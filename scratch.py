## Will need to source spark env vars prior to running


## YOU NEED TO ADJUST THE YARN APP MEMORY ALLOCATION BEFORE THIS WILL RUN
## IT SAYS THAT IT IS REVERTING TO DEFAULT OF 630mb AND THE TASK HERE IS ASKING FOR 1.4gb
## SOMETHING IN THE YARN CONFIGS SHOULD FIX THIS...
## LOOK FOR TOTAL 'AM' ALLOCATION OR SOMETHING LIKE THAT


import os
from pyspark import SparkContext


## For most data transformation think of each function as being applied to a row
def clean_spark_data(row_data):
    return([clean_row.replace('"', '') for clean_row in row_data])


## Start spark context

sc = SparkContext(master='yarn', appName="R-Log-Test")

## Pull in text data from hdfs
data = sc.textFile("{}/user/pi/pyspark_practice/2020-11-01.csv".format(os.environ['NAMENODE_PATH']))
## Split data into a series of row 'arrays'
rows = data.map(lambda x: x.split(',')).map(clean_spark_data)

## Cleaning needs:
##      -remove all quotation marks generated by spark
##          treats all data as quoted strings
##          Handled with the cleaning function, but could also be passed to the second map arg as [...].map(lambda x: [clean_row.replace('"','') for clean_row in x])

print(rows.take(5))
